#!/usr/bin/env python

import configparser
import psycopg2
import tweepy
import json
from datetime import datetime
from datetime import timedelta
import os
import shutil

def stream_tweets():
    """
    Establishes connection to Twitter using API credentials and StdOutListener()
    class. Filters Twitter stream for tweets containing any of the specified 
    earthquake keywords in the Postgres "keyword" table.
    """
    # Get streaming API user credentials
    consumer_key = config.get('TWITTER', 'twitter_apikey')
    consumer_secret = config.get('TWITTER', 'twitter_apisecret')
    access_token = config.get('TWITTER', 'twitter_accesstoken')
    access_token_secret = config.get('TWITTER', 'twitter_accesstoken_secret')

    # Authenticate Twitter API connection
    l = StdOutListener()
    try:
        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
        auth.set_access_token(access_token, access_token_secret)
        stream = tweepy.Stream(auth, l)
    except Exception as e:
        f.write("%s: Error encountered while connecting to Twitter API - %s\n" 
                % (str(datetime.now()), e))
        f.flush()
 
    # Get earthquake keywords
    try:
        f.write("%s: Connected to tweet stream.\n" % str(datetime.now()))
        f.flush()

        cur.execute("SELECT title FROM keyword;")
        quake_words = cur.fetchall()
    except Exception as e:
        f.write("%s: Error encountered while fetching earthquake keywords: %s\n" 
                % (str(datetime.now()), e))
        f.flush()

    keywords = []
    for word in quake_words:
        keywords.append(word[0])

    # Filter Twitter Streams to capture data by certain keywords
    stream.filter(track=keywords)

class StdOutListener(tweepy.StreamListener):
    def on_status(self, tweet):
        """
        Called by Twitter stream created in stream_tweets() method.
        Checks that new tweets coming through stream listener are not 
        retweets. If not a retweet, will call process_tweet().

        self: required parameter for StdOutListener() methods
        tweet: new tweet coming in through listener (TWEET object)
        """
        # Do not process retweets
        if hasattr(tweet, 'retweeted_status'):
            return
        process_tweet(tweet)

    def on_error(self, status_code):
        """
        Detects errors coming through stream listener; writes to
        logfile if error code corresponds to a rate limiting error 
        and keeps the stream running or all other error codes.

        self: required parameter for StdOutListener() methods
        status_code: error code returned by stream listener (INT)
        """
        if status_code == 420:
            f.write("%s: We are being rate limited.\n" % str(datetime.now()))
            f.flush()
            return False

def process_tweet(tweet):
    """
    Called by on_status() in StdOutListener() class when a new 
    tweet is filtered through Twitter stream. Takes TWEET object 
    and reads it into a JSON object, and uses both objects to read
    important information about tweet into dictionary called 
    tweet_dict. Finally, calls insert_from_dict() method.

    tweet: TWEET object containing descriptive information about
           earthquake tweet.
    """
    # Get tweet into json format
    tweet_json_string = json.dumps(tweet._json)
    tweet_json = json.loads(tweet_json_string)

    # Read data into dictionary
    tweet_dict = {}
    tweet_dict = {'twitter_date' : tweet.created_at,
                  'twitter_id' : tweet.id_str,
                  'location_string' : tweet.user.location, 
                  'text' : tweet.text, 
                  'location_lon' : None,
                  'location_lat' : None,
                  'coordinate_type' : None,
                  'media_type' : None, 
                  'media_display_url' : None,
                  'word_count' : 0,
                  'lang' : tweet.lang,
                  'time_zone' : tweet.user.time_zone,
                  'utc_offset' : tweet.user.utc_offset}

    # Take out single quotes from text to prevent sql error
    tweet_dict['text'] = tweet_dict['text'].replace("'", "")

    # Truncate long text for database
    tweet_dict['text'] = tweet_dict['text'][:290]
  
    # Look for tweet coordinates
    if tweet.coordinates is not None:
        tweet_dict['coordinate_type'] = 'message'
        tweet_dict['location_lon'] = tweet.coordinates['coordinates'][0]
        tweet_dict['location_lat'] = tweet.coordinates['coordinates'][1]
    elif tweet.place is not None:
        tweet_dict['coordinate_type'] = 'profile'
        # Get bounding box coordinates, average lat and lon to get one pair
        longitude_sum = 0.00
        latitude_sum = 0.00
        for i in range(0, 4):
           this_longitude = tweet_json['place']['bounding_box']['coordinates'][0][i][0]
           this_latitude = tweet_json['place']['bounding_box']['coordinates'][0][i][1] 
           longitude_sum = this_longitude + longitude_sum
           latitude_sum = this_latitude + latitude_sum
        tweet_dict['location_lon'] = longitude_sum / 4
        tweet_dict['location_lat'] = latitude_sum / 4

    # Check for photos or videos
    if 'media' in tweet.entities:
       tweet_dict['media_type'] = tweet.entities['media'][0]['type']
       tweet_dict['media_display_url'] = tweet.entities['media'][0]['media_url_https']
    
    # Get word count
    words = tweet.text.split()
    tweet_dict['word_count'] = len(words)

    # Check that no items in dictionary are empty
    for key, value in tweet_dict.items():
       if value is None:
           tweet_dict[key] = 'null'

    global tweetcount
    global tweetrate_time
    
    # Add to database
    insert_from_dict(tweet_dict)
    tweetcount += 1

    # Record tweets per 5 minutes in log file
    if datetime.now() >= (tweetrate_time + timedelta(minutes=5)):
         f.write("%s: Recording %i tweets every 5 minutes.\n" 
                 % (str(datetime.now()), tweetcount))
         f.flush()
         tweetcount = 0
         tweetrate_time = datetime.now()

    # Check if log file is more than a day old
    logfile_age = timedelta(days=1)
    check_logfile(logfile_age)

def insert_from_dict(tweet_dict):
    """Called by process_tweet(). Takes tweet dictionary and inserts into 
       "message" table in Postgres. 
       
       tweet_dict: Tweet dictionary with the following fields: 
                   (STRING unless otherwise noted)
                   - twitter_date: Date tweet was created 
                   - location_string: Location associated with user profile
                   - text: Tweeted text
                   - location_lon: Longitude coordinate of tweet, either from 
                     user profile or tweet itself.
                   - location_lat: Latitude coordinate of tweet, either from 
                     user profile or tweet itself.
                   - coordinate_type: source of tweet coordinates - either
                     'message' or 'profile'
                   - media_type: Type of additional media included in tweet,
                     e.g. 'photo', 'video'
                   - media_display_url: URL address of additional media.
                   - word_count: Number of words in tweeted text. (INT)
                   - lang: Two-character string describing language tweet was 
                     written in.
                   - time_zone: String describing user's time zone.
                   - utc_offset: String of an integer describing user's UTC offset.
    """ 
    date_created = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.00')
    lang = tweet_dict['lang']
    location_lat = tweet_dict['location_lat']
    location_lon = tweet_dict['location_lon']
    location_string = tweet_dict['location_string']
    location_source = tweet_dict['coordinate_type']
    media_display_url = tweet_dict['media_display_url']
    media_type = tweet_dict['media_type']
    message_date = tweet_dict['twitter_date'].strftime('%Y-%m-%d %H:%M:%S.00')
    message_id = tweet_dict['twitter_id']
    message_source = "Twitter"
    message_text = tweet_dict['text']
    time_zone = tweet_dict['time_zone']
    utc_offset = tweet_dict['utc_offset']
    word_count = tweet_dict['word_count']

    try:
        query = ("""INSERT INTO message (date_created, lang, message_date, message_id, 
                 message_source, message_text, time_zone, utc_offset, word_count) 
                 VALUES ('%s', '%s', '%s', '%s', '%s', '%s', '%s', %s, %i);""" % 
                 (date_created, lang, message_date, message_id, message_source, 
                  message_text, time_zone, utc_offset, word_count))
        cur.execute(query)
        if location_string != 'null':
            cur.execute("""UPDATE message SET location_string = %s WHERE
                             message_id = %s;""", (location_string, message_id))
        if media_type != 'null':
            cur.execute("""UPDATE message SET media_display_url = %s, 
                             media_type = %s WHERE message_id = %s;""",
                             (media_display_url, media_type, message_id))
        if location_lon != 'null' and location_lat != 'null':
            cur.execute("""UPDATE message SET location_source = %s, location_lon = 
                        %s, location_lat = %s WHERE message_id = %s;""" , 
                        (location_source, location_lon, location_lat, message_id))
    except Exception as e:
        f.write("%s: Error encountered while adding tweet to message table - %s\n"
                % (str(datetime.now()), e))
        f.flush()

def check_logfile(logfile_age):
    """
    Called by process_tweet(). Looks at creation date of logfile and checks if logfile
    is older than input parameter logfile_age. If it is, the logfile will be copied into
    /log/archive/ directory, removed from /log/ directory, and replaced with a brand new     
    log file. This is done to prevent logfiles from becoming too large. 

    logfile_age: DATETIME object.
    """

    try:
        # Check if logging functionality turned on, else exit method
        if loggingon:
            global f

            # If /log/ and /log/archive/ directories exist, create them
            if not os.path.exists('log/archive'):
                os.makedirs('archive')

            # Check first line of logfile for file creation time 
            with open(logfile, 'r') as fread:
               first_line = fread.readline().rstrip()
       
            colon = first_line.index(': ') 
            createdate_string = first_line[:colon]
            createdate = datetime.strptime(createdate_string, '%Y-%m-%d %H:%M:%S.%f')

            # Check if logfile is more than a day old
            if datetime.now() >= (createdate + logfile_age):
                archive_folder = os.path.join(log_folderpath, 'archive')
                log_date = createdate.strftime("%m-%d")
           
                f.write("%s: Copying logfile to archive and deleting this logfile.\n" 
                        % datetime.now()) 
                f.flush()

                # Copy logfile into archive
                shutil.copyfile(logfile, os.path.join(archive_folder, 
                                config.get('SETUP', 'logfile') + '.' + log_date))
            
                # Delete first logfile
                os.remove(logfile)

                # Create new logfile
                f = open(logfile, 'at')
                f.write("%s: Logfile created.\n" % datetime.now()) 
                f.flush()

    except Exception as e:
        f.write("Error in check_logfile: %s\n" % str(e))

if __name__ == '__main__':
    # Read in config file
    homedir = os.path.dirname(os.path.abspath(__file__))
    configfile = os.path.join(homedir, 'configFetcher.ini')
    if not os.path.isfile(configfile):
        print("Config file '%s' does not exist. Exiting\n" % configfile)
        sys.exit(1)

    config = configparser.ConfigParser()
    config.readfp(open(configfile))
   
    missing = [] 
    if not config.has_section('SETUP'):
        print("Config file '%s' is missing section 'SETUP'. Exiting\n" % configfile)
        sys.exit(1)
    for option in ['logging_on', 'logfile']:
        if not config.has_option('SETUP', option):
            missing.append(option)
        if len(missing):
            print("Config file '%s' is missing SETUP options '%s'. Exiting\n" % 
                  (configfile, ','.join(missing)))
            sys.exit(1)
    
    # Set up log file
    loggingon = bool(int(config.get('SETUP', 'logging_on')))

    if not os.path.exists('log'):
        os.makedirs('log')
    log_folderpath = os.path.join(homedir, 'log')
    logfile = os.path.join(log_folderpath, config.get('SETUP', 'logfile'))
 
    if loggingon:
        if os.path.exists(logfile):
            f = open(logfile, 'at')
        else:
            f = open(logfile, 'at')
            f.write("%s: Logfile created.\n" % datetime.now())

    # Connect to test database
    prefix = 'testdb'
    port = config.get('DATABASE', prefix+'_port')
    user = config.get('DATABASE', prefix+'_user')
    dbname = config.get('DATABASE', prefix+'_name')
    password = config.get('DATABASE', prefix+'_password')

    conn = psycopg2.connect(dbname=dbname, user=user, port=port, password=password)
    cur = conn.cursor()
    conn.autocommit = True

    # Variables used later on
    tweetrate_time = datetime.now()
    tweetcount = 0

    stream_tweets()

    # Close database connections
    conn.close()
    cur.close()
    if loggingon:
        f.close() 
